# Kimi K2 ist WAHNSINN... (Open-Source ist ZUR√úCK!)

Ich habe k√ºrzlich ein Video angesehen und fand es ziemlich aufschlussreich. Um den Inhalt besser zu verstehen und zu teilen, habe ich **[Viddo](https://viddo.pro/)** verwendet, um das Video in einen strukturierten Artikel umzuwandeln, der als Referenz f√ºr diese Analyse diente.

**Original Video:** [Video ansehen](> - Dies k√∂nnte der n√§chste Deepseek-Moment sein.
> - Ein chinesisches Unternehmen hat gerade ein Open-Source-Modell namens Kimmy K2 ver√∂ffentlicht.
> - Das Modell revolutioniert die Branche mit seiner glatten Trainingsverlustkurve.
> - Es basiert auf einer Billion Tokens und bietet unglaubliche Leistung.
> - Kimmy K2 ist f√ºr Kodierung, reasoning und autonomes Probleml√∂sen konzipiert.

Dies k√∂nnte der n√§chste **Deepseek-Moment** sein. Ein chinesisches Unternehmen hat gerade ein weiteres **Open-Source-Modell** namens **Kimmy K2** ver√∂ffentlicht, das die Branche im Sturm erobert. Der Grund daf√ºr ist dieses Diagramm hier, das ist die **Trainingsverlustkurve**, und die Leute sind so √ºberrascht, wie glatt sie ist. Typischerweise sieht man all diese Spitzen, die Probleme verursachen, die man beheben muss, aber bei **Kimmy** war es fast perfekt.

Und das besonders Coole daran ist, dass es auf einer **Billion Tokens** basiert. Das ist ein massive Modell! Sie haben einen neuen Ansatz entwickelt, den sie implementiert haben, und es funktionierte sehr √§hnlich, wie **Deepseek** unglaublich effizient war - effizienter, als wir es je zuvor gesehen hatten. Aber das ist gro√üartig. Es hat sich wirklich gut trainiert.

Was bedeutet das eigentlich? Nun, zun√§chst einmal ist es ein **massives Open-Source-Modell**, das unglaublich gut funktioniert. **Kimik K2** ist ein hochmodernes gemischtes Experten-Sprachmodell mit **32 Milliarden aktiven Parametern** und **1 Billion insgesamt Parametern**.

Und hier liegt der Schl√ºssel. Es wird mit dem **Muon-Optimizer** trainiert und erzielt au√üergew√∂hnliche Leistungen in den Bereichen Grenzwissen, reasoning und Kodierungsaufgaben, w√§hrend es sorgsam f√ºr **Agentenf√§higkeiten** optimiert ist. Daher ist es unglaublich gut in der Kodierung, unglaublich gut im Multi-Agenten-Bereich und unglaublich gut im Tool-Calling. Vortrainiert auf einer Billion Tokens mit **null Trainingsinstabilit√§t**.

Sie haben diesen **Muon Clip-Optimizer** in einem beispiellosen Ma√üstab eingesetzt und neuartige Optimierungstechniken entwickelt, um Instabilit√§ten beim Hochskalieren zu beheben. Dieses Modell ist speziell f√ºr **Tool-Nutzung**, reasoning und autonomes Probleml√∂sen konzipiert.

Laut **Crystal**, die im Kimmy **Moonshot-Team** ist, unterst√ºtzt Kimmy bis zu **2 Millionen Tokens** im Kontextfenster. Sie sagte, das gesamte Labor, das KI-Labor, besteht nur aus **200 Personen**. Die Kimmy-Website unterst√ºtzt es noch nicht direkt, aber sie haben es getestet.

Es gab ein wenig Qualit√§tsverlust, aber es ist sehr wahrscheinlich. Also haben wir zwei Versionen: die Basisversion und die Instruct-Version, aber wisst ihr, was wir nicht haben? **Die reasoning-Version.** Jetzt, da es Open Source ist, hat jeder Zugriff darauf. Viele reasoning-Versionen von **Kimmy K2** werden bald kommen.

Schauen wir uns die Benchmarks an; sie sind **stunning**. Dies ist ein Modell auf Grenzniveau. Hier ist **SWI Bench**, das Kimmy K2 Instruct best√§tigt und **Deepseek**, **Quen** und **GPT4.1** √ºbertrifft, nur knapp hinter **Cloud4 Opus**, das als das beste Kodierungsmodell auf dem Planeten bekannt ist.

**SWED Bench Multilingual** √ºbertrifft einmal mehr all diese anderen Modelle und kommt direkt hinter **Cloud 4 Sonnet**. **Live Codebench** √ºbertrifft tats√§chlich **Cloud 4 Opus**, **Gemini 2.5 Flash** kommt mit **53.7 OJ** Bench und √ºbertrifft alle anderen Modelle auf der Liste.

Hier ist **Amy20025** f√ºr Mathematik, die an erster Stelle steht, vor **Cloud4 Opus** und **Gemini 2.5 Flash**, und das alles ohne eine reasoning-Version. **GPQA Diamond** kommt an **Nummer 175.1**, √ºber **Cloud4 Opus** und **Gemini 2.5 Flash**. Ich bin so gespannt darauf, dieses Modell zu testen, und es ist vollst√§ndig Open Source.

Offene Gewichte. Der Trainingsprozess war Open Source. Sie werden bald ein **Forschungspapier** dazu ver√∂ffentlichen. Es ist **erstaunlich**. Wenn du die vollst√§ndige Sammlung von Benchmarks haben m√∂chtest, gehe zu ihrer **Hugging Face-Karte**. Sie haben alles hier: **Ader, Polyglot, ACE Bench, Amy 20, 2425 Math 500, Polymath, GPQA Diamond**, **Humanity's Last Exam, MMLU Pro** und so vieles mehr.

Und es gibt zahlreiche Inferenzanbieter, die das bereits laden und bereitstellen. Wenn du das Beste aus **Kimmy K2** und anderen Modellen herausholen m√∂chtest, musst du dein **Prompt Engineering optimieren**. Du kannst dies mit dem **Humanity's Last Prompt Engineering Guide** tun, der von mir und meinem Team erstellt wurde.

Es ist v√∂llig kostenlos und es lehrt dich alle besten Prompts, Engineering-Tipps und Tricks. Link in der Beschreibung unten, und du kannst auch direkt √ºber Kimmy Inferenz erhalten. Es kostet **$0,15 pro Million Eingabetokens**, **$0,60 ohne Cache** und **$2,50 pro Ausgabetokens**.

Die Gewichte, der technische Blog und die **GitHub-Seite** sind jetzt alle offen und verf√ºgbar, und wenn du es einfach ohne die API ausprobieren m√∂chtest, probiere sofort **Kimmy AI**. Also ein paar Worte von Branchenexperten und KI-F√ºhrern.

Hier ist **Sebastian Rashka**: Kimmy K2 ist im Grunde **Deepseek V3**, aber mit weniger K√∂pfen und mehr Experten. Und nochmals, ich kann es kaum erwarten, bis sie tats√§chlich **Chain of Thought** und reasoning-F√§higkeiten geben.

**Ychn Jin** sagt: Heilige Schei√üe, Kimmy K2 wurde auf **15,5 Billionen Tokens** vortrainiert, wobei **Muon Clip** verwendet wurde. Mit Trainingsspitzen sind sie offiziell auf **1 Billion Parameter LLM**-Niveau skaliert. Viele haben bezweifelt, dass es skalieren kann, aber hier sind wir.

**DD** sagt: China hat gerade das beste Open-Source-Modell f√ºr Kodierung und agentenbasierte Tool-Nutzung ver√∂ffentlicht. Kimik K2 hat einen verr√ºckten **65,8 auf Sweebench** erreicht, √ºberpr√ºft, es ist so g√ºnstig wie **Gemini Flash** bei **$0,6 pro Million Eingaben** und **$2,5 pro Million Ausgaben**.

Und er gibt ein Beispiel: Es l√∂st diese Datenanalysaufgabe in Python im einmaligen Versuch und erstellt eine Website f√ºr **ein paar Cent**. Schau dir diese unglaubliche harte **Maru** an. Jede **ML-Engineer's Traumverlustkurve** ist genau dort. Sie geht einfach runter. Keine Spitzen, keine Unterbrechungen.

Hier ist ein weiterer Prompt. Hier ist ein Beispiel daf√ºr. Also das XAI-Hauptquartier kurz bevor sie ihr gr√∂√ütes Modell **Grok 4** herausgeben: belebtes Gebiet, viele Menschen arbeiten im gro√üen B√ºro. Also das ist **Grok 3**, das ist **Grok 4** und das ist **Kimmy K2**. Es sieht beeindruckend aus.

Und Kimmy K2 ist jetzt auf **OpenRouter** verf√ºgbar. Also, wenn du es ausprobieren und die API nutzen m√∂chtest, mach weiter. Es ist bereit.

**Ethan Moik**, Professor Wharton: Kimmy K2 scheint ein sehr gutes und riesiges und seltsames **Open-Weights-Modell** zu sein, das der neue F√ºhrer in den offenen **LLMs** sein k√∂nnte. Es schl√§gt die geschlossenen Modelle an der Grenze in meinem seltsamen Test nicht, aber es hat noch keinen Reasoner.

Und schau dir das an. **Ani Hunnon** hat Kimmy K2 **1 Billion Modell 4bit Quant** zum Laufen auf **2512 Gigabyte N3 Ultras** mit **MLX LM** gebracht. Also da haben wir es, es l√§uft und ich muss sagen, ziemlich schnell.

Hier ist **Cedric**, der sagt, Kimmy K2 hat **Minecraft f√ºr das Web** in einem Shot abgeschlossen. Das hat mich **vier Tage** und **sechs Versuche** gekostet, um **Gemini 2.5 Pro** zu bekommen. Wow.

Und nat√ºrlich plante er den **Liberator**, wieder einmal **Jailbrok**. Nichts ist vor **Pliny** sicher.

Das war's f√ºr heute. Wenn du sehen m√∂chtest, wie ich dieses Modell gr√ºndlich teste, lass es mich in den Kommentaren wissen. Wenn dir dieses Video gefallen hat, ziehe in Betracht, einen Like zu geben und zu abonnieren, und ich sehe dich beim n√§chsten Mal.)
**Referenzartikel:** [Auf Viddo ansehen](https://viddo.pro/zh/video-result/5768f631-b053-41de-9597-7d5dff151298)

---

Ich habe k√ºrzlich ein Video angesehen, das sich mit einem Open-Source-Gro√ümodell namens **Kimmy K2** besch√§ftigt. Nachdem ich es gesehen hatte, konnte ich einfach nicht widerstehen, etwas dazu zu schreiben, besonders weil die Leistung dieses Modells wirklich √ºberw√§ltigend ist.

---

**üí°Kernzusammenfassung**

Kimmy K2 ist ein Open-Source-Gro√ümodell, das von einem chinesischen Team ver√∂ffentlicht wurde, das nicht nur √ºber eine erstaunliche Anzahl von Parametern verf√ºgt, sondern dessen Trainingsprozess auch nahezu perfekt und fehlerfrei ist. Es zeigt herausragende Leistungen bei der Kodierung, dem Reasoning und der Zusammenarbeit von mehreren Agenten und wird als m√∂glicher n√§chster "Deepseek-Moment" betrachtet. Seine Ver√∂ffentlichung hat das Verst√§ndnis der Leute f√ºr die Leistung von Open-Source-Modellen radikal ver√§ndert.

---

**üî•Schl√ºsselzitate**

- "Die Verlustkurve eines jeden ML-Ingenieurs, die einfach so sauber nach unten geht."
- "Kimmy K2, nicht nur Open Source, sondern ohne Trainingsinstabilit√§t."
- "Es l√∂st die Datenanalysaufgabe in Python im einmaligen Versuch und erstellt nebenbei eine Website."

---

Um ehrlich zu sein, war ich ein wenig immun gegen√ºber Open-Source-Modellen, schlie√ülich kommen in letzter Zeit st√§ndig neue Modelle heraus. Aber das Erscheinen von Kimmy K2 hat mich wirklich √ºberrascht. Die Verlustkurve w√§hrend des Trainings ist so glatt wie eine Seide, ohne eine einzige Welle ‚Äì das ist im Bereich der Gro√ümodelle fast unvorstellbar. Und das alles wurde von einem Labor mit nur 200 Personen erreicht; klein im Umfang, aber gro√ü in den Ambitionen.

Noch schockierender ist seine Beherrschung von Codeaufgaben: von der Datenanalyse in Python bis hin zur Erstellung von Webseiten, es trifft fast direkt ins Schwarze. Man kann kaum nicht dar√ºber nachdenken: Ist unser Vorstellungsverm√∂gen √ºber Open-Source-Modelle vielleicht zu lange begrenzt gewesen? Kimmy K2 k√∂nnte genau die Art von Durchbruch sein, die wir brauchen.

Ein weiterer Punkt, der mich besonders ber√ºhrt: Das Team hat zuerst keine Reasoning-Version erstellt, sondern die Basisversion ver√∂ffentlicht, um der Community zu erm√∂glichen, weiterzuentwickeln. Dieser Ansatz "Ich bereite die Bahn vor, ihr k√∂nnt fliegen" ist √§u√üerst offen und voller Vertrauen.

Hinter Kimmy K2 gibt es viele interessante Punkte, sei es der Muon-Optimizer, das 2M-Kontextfenster oder die detaillierte Optimierung der Agentenf√§higkeiten. Noch wichtiger ist, dass es ein Feuer im Open-Source-Community entfacht hat. Dieses Modell stellt nicht nur einen technischen Durchbruch dar, sondern sendet ein Signal aus ‚Äì **wirklich kraftvolle Dinge k√∂nnen auch aus kleinen Teams und aus Open Source kommen.**

Das hat mich noch mehr an eine Sache glauben lassen: Der n√§chste Paradigmenwechsel k√∂nnte tats√§chlich nicht aus dem Silicon Valley kommen.

---

**M√∂chtest du deine eigenen Videos in Artikel umwandeln?** Probiere **[Viddo](https://viddo.pro/)** - die KI-gest√ºtzte Plattform, die Videoinhalte in ansprechende, lesbare Artikel in Minuten umwandelt. Perfekt f√ºr Content-Ersteller, Educators und Fachleute, die ihre Videoinhalte f√ºr Blogs, soziale Medien oder Dokumentationen umnutzen m√∂chten.

[üöÄ Beginne mit der Umwandlung von Videos mit Viddo](https://viddo.pro/)