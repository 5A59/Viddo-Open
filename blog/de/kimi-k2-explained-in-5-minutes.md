# Kimi K2 in 5 Minuten erklÃ¤rt

Neulich habe ich ein Video angesehen und fand es sehr aufschlussreich. Um die Inhalte besser zu verstehen und zu teilen, habe ich **[Viddo](https://viddo.pro/)** verwendet, um das Video in einen strukturierten Artikel umzuwandeln, der als Referenz fÃ¼r diese Analyse diente.

**Original Video:** [Video ansehen](> - Kimike K2 von Moonshot sorgt fÃ¼r Aufsehen in der KI-Branche.
> - Viele bevorzugen weiterhin Modelle wie Claude und Gemini.
> - Das Potenzial von Kimike K2 liegt in seiner innovativen Architektur.
> - Kostenfaktoren beeinflussen die Annahme von Kimike K2.
> - Die Entwicklung von Open-Source-Modellen kÃ¶nnte das Wettbewerbsumfeld verÃ¤ndern.

Kimike K2 wurde von Moonshot verÃ¶ffentlicht und hat groÃŸen Einfluss auf die KI-Branche. Und du magst sagen, nein, ich benutze Claude und Gemini, und sie funktionieren einwandfrei, also warum sollte ich mich wirklich dafÃ¼r interessieren? 

Die meisten Leute haben wahrscheinlich noch nie von Kimike K2 oder sogar Moonshot gehÃ¶rt, da der Markt im kommerziellen Bereich Ã¼berwiegend von OpenAI, Anthropic und Gemini dominiert wird. Aber es gibt einen Grund, warum die Open-Source-Modell-Community von Modellen wie Kimikke 2 begeistert ist. 

Der Grund, warum Kimikke 2 nicht weit verbreitet ist, hat grÃ¶ÃŸtenteils mit dem Wort **groÃŸ** im groÃŸen Sprachmodell zu tun, was bedeutet, dass diese Open-Source-Modelle immer noch zu groÃŸ sind, um lokal betrieben zu werden und die gleichen Ergebnisse wie die leistungsstÃ¤rksten Modelle zu erzielen. 

Zum Beispiel hat Kimi K2 insgesamt **1 Billion Parameter**, was mit anderen Grenzmodellen wie GPT, Gemini und Claude vergleichbar ist. Der **Wettbewerbsvorteil**, den diese Modelle gegenÃ¼ber Kimikke 2 haben, beruht auf **Skaleneffekten**. Was ich damit meine, ist, dass man, um ein Milliarden-Parameter-Modell fÃ¼r die Millionen von Nutzern, die sie haben, laufen zu lassen, eine **groÃŸ angelegte Infrastruktur** benÃ¶tigt, um es zu unterstÃ¼tzen. 

Und leider muss man fÃ¼r den Betrieb von Kimikk 2 mindestens **25.000 $** ausgeben, um eine Nvidia H100-Karte zu kaufen, Ã¤hnlich wie beim Kauf eines brandneuen Autos. Und genau wie beim Besitzen eines Autos muss man Benzin einkaufen, um es zu betreiben. Und H100 kann bis zu **90 $ pro Monat** kosten, um es lokal zu betreiben, wobei man etwa **0,7 Kilowatt** verbraucht. 

Also fragst du dich vielleicht an diesem Punkt, was hier wirklich so wichtig ist? Sieht es so aus, als wÃ¤re Kimikk 2 immer noch nicht wirklich weit genug? Sicher, die Mathematik begÃ¼nstigt weiterhin die kommerziellen Modelle, denn du zahlst nur eine AbonnementsgebÃ¼hr von **200 $ pro Monat** pro Entwickler, um **unbegrenzte API-Aufrufe** fÃ¼r die Nutzung von hochmodernen Modellen wie Claude zu haben. Das sind nur **2.400 $ pro Jahr**, was deutlich weniger ist, als **25.000 $** fÃ¼r den Betrieb von Kimik K2 auszugeben. 

Wenn man es aus der Perspektive eines Unternehmens betrachtet, beginnt jedoch die Mathematik allmÃ¤hlich in die andere Richtung zu kippen. Zum Beispiel kÃ¶nnte die **25.000 $ Hardware**, die ein Unternehmen erworben hat, als anfÃ¤ngliche Investition betrachtet werden, die man im Voraus als Investitionsausgabe bezahlt und steuerlich abgeschrieben werden kann. 

Oben drauf werden im folgenden Jahr nur **1.080 $ pro Jahr** an Stromkosten fÃ¼r die Inferenz anfallen, was weit weniger ist als die **2.400 $** und mÃ¶glicherweise noch weniger, wenn die gleiche Hardware zwischen zwei Entwicklern geteilt wird. 

So siehst du, warum es so viel Aufregung um Modelle wie Kimikk K2 gibt, denn der Open-Source-Modellmarkt holt allmÃ¤hlich auf, sodass **mit hochmodernen vergleichbaren Modellen** wie Kimikk K2 nun lokal gearbeitet werden kann. Die offensichtliche nÃ¤chste Frage ist also **wie?**

Wie schafft es Kimik K2, so gut gegen hochmoderne Modelle wie Claude GPT und Gemini abzuschneiden? Die Architektur von Kimikk 2 basiert auf dem, was als **MOE** oder **Mixture of Experts** bezeichnet wird. Die meisten kommerziellen Modelle wie GPT und Claude sind sogenannte **dichte Modelle**, wÃ¤hrend Kimikk 2 ein **dÃ¼nnes Modell** ist. 

Ein dichtes Modell ist dein typisches vorwÃ¤rtsgerichtetes neuronales Netzwerk, das das gesamte Modell aktivieren kann, um Token zu verarbeiten, wÃ¤hrend dÃ¼nne Modelle nur einen Abschnitt oder wenige Abschnitte des Modells aktivieren, um deine Token zu verarbeiten. Deshalb aktiviert Kimi K2, auch wenn es **1 Billion Parameter** groÃŸ ist, tatsÃ¤chlich nur **8 Abschnitte**, in diesem Fall 8 Experten pro Token. 

Das Modell hat insgesamt **384 Experten**, die zusammen etwa **1 Billion Parameter** ausmachen, und das macht die Inferenz viel schneller, da es nur **32 Milliarden aktive Parameter** auf einmal nutzt. 

Kimik 2 wurde auch auf Aktionen trainiert, was bedeutet, dass es speziell darauf trainiert wurde, bessere Tool-Anfragen zu machen. Und ich denke, das ist ein sehr wichtiger Punkt, an dem **LLM-Benchmarking**, das typischerweise verwendet wird, um die rohe Intelligenz eines bestimmten Modells zu messen, jetzt erweitert wird, um nach Modellen zu suchen, die **ressourcenreicher** sind, indem sie externe Dienste und Tools nutzen, um bessere Aktionen zu erstellen. 

Moonshot erkennt und hat Kimik K2 speziell auf **SIM-Tool-Nutzung** trainiert, um zu lernen, wie man beim Einsatz der richtigen Tools fÃ¼r **verschiedene Zwecke** und **verschiedene Kontexte** effizient vorgeht. Und dies wird groÃŸe Dividenden bringen, da die Branche sich zunehmend in Richtung **MCP** und **A2A** oder **Agent zu Agent Netzwerke** entwickelt, die stark auf externe AbhÃ¤ngigkeiten angewiesen sind. 

Meine nÃ¤chste Frage ist dann, wie sich Kimik 2 im Hinblick auf das verhÃ¤lt, was im **Januar 2025** geschah, als DeepSeek 1 erstmals verÃ¶ffentlicht wurde und die Welt erschÃ¼tterte? Erinnerst du dich an die AnkÃ¼ndigung von DeepSea, die als **Chat GPT-Killer** bezeichnet wurde und an der die BÃ¶rse schlieÃŸlich eine Billion Dollar verlor? 

Wird die Kosten fÃ¼r den Betrieb dieser Modelle letztlich vergleichbar mit der Verwendung von Grenzmodellen wie **OpenAIs GPT**, **Anthropics Claude** und **Googles Gemini** sein? Bei jeder grÃ¶ÃŸeren VerÃ¶ffentlichung wie DeepSeek1 oder Kimik K2 wird der Wettbewerbsvorteil, den diese Unternehmen derzeit genieÃŸen, schrittweise beseitigt. 

Und ich denke, das ist der Grund, warum LLM-Anbieter erkennen, dass ihr Wettbewerbsvorteil nicht von Dauer ist, weshalb wir sehen, dass sie ihre Produktangebote auf angrenzende Produkte wie **KI-Code-Editoren**, **KI-Webbrowser** und **KI-Chat-Anwendungen** usw. erweitern, da dies ihren Wettbewerbsvorteil etwas lÃ¤nger aufrechterhÃ¤lt.

Wenn wir also demnÃ¤chst Open-Source-Modelle wie Kimik K2 sehen, die immer zugÃ¤nglicher und nÃ¼tzlicher werden, werden wir beobachten, wie sich die **Industrie verÃ¤ndert**. Und es wird interessant sein zu sehen, wo **Self-Hosting** zur Norm wird. Oder vielleicht werden die kommerziellen Modelle die Branche weiterhin dominieren, weil sie einfach mehr **Geld investieren** kÃ¶nnen, um schnellere Innovationen zu fÃ¶rdern.
**Referenzartikel:** [Auf Viddo ansehen](https://viddo.pro/zh/video-result/72068e82-62a8-4ef9-b6f8-09eaae0e0b0a)

---

Neulich habe ich ein Video gesehen, das Ã¼ber das von Moonshot herausgebrachte Kimike K2-Modell spricht. Nachdem ich es gesehen habe, war ich ziemlich beeindruckt, besonders von dem subtilen, aber starken Einfluss, den es auf das gesamte Ã–kosystem der KI-Modelle haben kÃ¶nnte.

---

**âš¡ï¸ Die Inspiration hinter Kimike K2 ist klar und mutig:** Obwohl es nicht so bekannt ist wie GPT oder Claude, zeigt es die MÃ¶glichkeit, dass die Open-Source-Community allmÃ¤hlich mit den kommerziellen Giganten aufholt und in einigen architektonischen Aspekten sogar radikaler ist. Es ist nicht nur ein technischer Wettlauf, sondern ein Machtspiel darÃ¼ber, â€wem die KI-Zukunft gehÃ¶rtâ€œ.

---

> **â€Eine Billion Parameter sind nicht dafÃ¼r da, um anzugeben, sondern um Genauigkeit und Effizienz in Einklang zu bringen.â€œ**  
> **â€Die MOE-Architektur lÃ¤sst K2 weniger Parameter aktivieren, lÃ¤uft aber schneller.â€œ**  
> **â€Ein Modell zu besitzen, ist wie ein Auto zu besitzen, nicht nur Fahrkarten zu kaufen.â€œ**

---

Ehrlich gesagt habe ich bisher nicht viel auf diese weniger mainstream Modelle geachtet, da Claude und GPT bereits die meisten meiner BedÃ¼rfnisse erfÃ¼llen. Aber dieses Video hat mich dazu gebracht, die â€Eigentumsfrageâ€œ neu zu Ã¼berdenken: Ist es einfacher, einen Dienst zu mieten, oder hat man mehr Freiheit, wenn man ein eigenes System besitzt?

Kimike K2 realisiert durch die Mixture of Experts-Architektur einen leichten Betrieb im Billionen-Parameter-Bereich. Obwohl die Hardwarebarriere noch absurd hoch ist (H100-Grafikkarten kosten schnell mal 20.000 bis 30.000 Dollar), ist es aus einer unternehmerischen Perspektive mÃ¶glicherweise nicht teurer als ein Abonnement fÃ¼r kommerzielle APIs. Wichtiger ist, dass das Modell selbst darauf trainiert ist, â€handlungsfÃ¤higerâ€œ zu sein â€“ es ist nicht nur intelligent, sondern weiÃŸ auch, wie man Werkzeuge einsetzt, um Aufgaben zu erfÃ¼llen.

Das ist der Punkt, der mich am meisten beeindruckt: **KÃ¼nftige KI wird nicht nur fÃ¼r Chats und Schreibarbeiten zustÃ¤ndig sein, sondern wird mehr wie ein â€arbeitsfÃ¤higerâ€œ digitaler Assistent sein.** Moonshot konzentriert sich darauf, â€wie man KI wirklich zum Arbeiten bringtâ€œ, was sehr ansprechend ist.

In diesem Moment wurde mir plÃ¶tzlich klar: Vielleicht betrachten wir nicht einfach ein â€weiteres Open-Source-Modellâ€œ, sondern wir erleben, wie sich die Wege von Open Source und Kommerz tatsÃ¤chlich kreuzen. Vielleicht kÃ¶nnen wir in ein oder zwei Jahren tatsÃ¤chlich die â€Hoheitâ€œ Ã¼ber KI zurÃ¼ckgewinnen. Das wÃ¤re die aufregendste Zukunft.

---

**MÃ¶chtest du deine eigenen Videos in Artikel umwandeln?** Probiere **[Viddo](https://viddo.pro/)** - die KI-gestÃ¼tzte Plattform, die Videoinhalte in ansprechende, lesbare Artikel in wenigen Minuten verwandelt. Perfekt fÃ¼r Inhaltsanbieter, Educators und Fachleute, die ihre Videoinhalte fÃ¼r Blogs, soziale Medien oder Dokumentationen nutzen mÃ¶chten.

[ğŸš€ Beginne mit der Umwandlung von Videos mit Viddo](https://viddo.pro/)